{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import TypedDict, Dict, List, Optional, Any\n",
    "from datetime import datetime, date, timedelta\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from api_config import get_openai_client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANONICAL_REGIONS = [\n",
    "    \"Вінницька\", \"Волинська\", \"Дніпропетровська\", \"Донецька\",\n",
    "    \"Житомирська\", \"Закарпатська\", \"Запорізька\",\n",
    "    \"Івано-Франківська\", \"Київська\", \"Кіровоградська\",\n",
    "    \"Луганська\", \"Львівська\", \"Миколаївська\", \"Одеська\",\n",
    "    \"Полтавська\", \"Рівненська\", \"Сумська\", \"Тернопільська\",\n",
    "    \"Харківська\", \"Херсонська\", \"Хмельницька\", \"Черкаська\",\n",
    "    \"Чернівецька\", \"Чернігівська\",\n",
    "    \"Київ\", \"м. Севастополь\", \"АР Крим\"\n",
    "]\n",
    "\n",
    "REGION_COORDS = {\n",
    "    \"Вінницька\": (49.233, 28.467),\n",
    "    \"Волинська\": (50.733, 25.317),\n",
    "    \"Дніпропетровська\": (48.467, 35.033),\n",
    "    \"Донецька\": (48.000, 37.800),\n",
    "    \"Житомирська\": (50.250, 28.650),\n",
    "    \"Закарпатська\": (48.617, 22.283),\n",
    "    \"Запорізька\": (47.833, 35.133),\n",
    "    \"Івано-Франківська\": (48.917, 24.700),\n",
    "    \"Київська\": (50.450, 30.517),\n",
    "    \"Київ\": (50.450, 30.517),\n",
    "    \"Кіровоградська\": (48.500, 32.267),\n",
    "    \"Луганська\": (48.567, 39.300),\n",
    "    \"Львівська\": (49.833, 24.017),\n",
    "    \"Миколаївська\": (46.967, 31.983),\n",
    "    \"Одеська\": (46.467, 30.717),\n",
    "    \"Полтавська\": (49.583, 34.550),\n",
    "    \"Рівненська\": (50.617, 26.250),\n",
    "    \"Сумська\": (50.900, 34.783),\n",
    "    \"Тернопільська\": (49.550, 25.583),\n",
    "    \"Харківська\": (49.983, 36.217),\n",
    "    \"Херсонська\": (46.633, 32.600),\n",
    "    \"Хмельницька\": (49.417, 26.967),\n",
    "    \"Черкаська\": (49.433, 32.050),\n",
    "    \"Чернівецька\": (48.283, 25.933),\n",
    "    \"Чернігівська\": (51.483, 31.283),\n",
    "    \"АР Крим\": (44.950, 34.100),\n",
    "    \"м. Севастополь\": (44.600, 33.533),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalState(TypedDict):\n",
    "    \"\"\"\n",
    "    Канонічний стан енергосистеми для кожного регіону та моменту часу\n",
    "    \"\"\"\n",
    "    # Ідентифікація\n",
    "    region: str\n",
    "    timestamp: str  \n",
    "    \n",
    "    # Дані ризику (з dataset_preparation)\n",
    "    air_raid_alert: int  # 0/1\n",
    "    alert_intensity: float  # 0.0 - 1.0\n",
    "    alert_duration_minutes: Optional[int]\n",
    "    \n",
    "    blackout_status: str  # \"none\", \"planned\", \"unplanned\", \"probability\"\n",
    "    blackout_probability: float  # 0.0 - 1.0\n",
    "    blackout_consumers: int\n",
    "    blackout_settlements: int\n",
    "    \n",
    "    # Погодні дані\n",
    "    weather_temp: Optional[float]\n",
    "    weather_wind_speed: Optional[float]\n",
    "    weather_wind_gusts: Optional[float]\n",
    "    weather_precipitation: Optional[float]\n",
    "    weather_snowfall: Optional[float]\n",
    "    weather_cloud_cover: Optional[float]\n",
    "    weather_pressure: Optional[float]\n",
    "    weather_extreme_flag: bool\n",
    "    \n",
    "    # Енергетичні дані (з data_electricity)\n",
    "    demand: Optional[float]\n",
    "    net_generation: Optional[float]\n",
    "    total_interchange: Optional[float]\n",
    "    \n",
    "    # Похідні ознаки\n",
    "    demand_rolling_mean: Optional[float]\n",
    "    demand_volatility: Optional[float]\n",
    "    generation_rolling_mean: Optional[float]\n",
    "    generation_volatility: Optional[float]\n",
    "    \n",
    "    # Метадані\n",
    "    data_completeness: float  # 0.0 - 1.0 (якість даних)\n",
    "    last_updated: str  # ISO timestamp\n",
    "    source_priority: Dict[str, int]  # Пріоритет джерел даних\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorState(TypedDict):\n",
    "    \"\"\"\n",
    "    Стан оркестратора - містить всю інформацію про процес синхронізації та обробки\n",
    "    \"\"\"\n",
    "    # Вхідні дані з різних джерел\n",
    "    risk_data: Dict[str, pd.DataFrame]  # Дані ризику по регіонах\n",
    "    energy_data: Dict[str, pd.DataFrame]  # Енергетичні дані по регіонах\n",
    "    \n",
    "    # Канонічний стан (синхронізований)\n",
    "    canonical_states: Dict[str, CanonicalState]  # key: f\"{region}_{timestamp}\"\n",
    "    \n",
    "    # Energy graph\n",
    "    energy_graph: Optional[nx.Graph]  # Граф енергосистеми\n",
    "    \n",
    "    # Контекст для LLM\n",
    "    llm_context: Optional[Dict[str, Any]]  # Зведений контекст для передачі LLM\n",
    "    should_call_llm: bool  # Чи потрібно викликати LLM\n",
    "    llm_response: Optional[str]  # Відповідь від LLM\n",
    "    \n",
    "    # Метадані\n",
    "    sync_timestamp: str  # Час синхронізації\n",
    "    regions_processed: List[str]  # Список оброблених регіонів\n",
    "    sync_errors: List[str]  # Помилки синхронізації\n",
    "    \n",
    "    # Результат\n",
    "    decision_required: bool  # Чи потрібне прийняття рішення\n",
    "    critical_regions: List[str]  # Регіони з критичним станом\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_risk_data(file_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Завантажує дані ризику з GNN_AGENT підготовки\n",
    "    Очікує колонки: region (або ua_region), date (або timestamp_utc), \n",
    "    Alerts (або alert_active), isDamaged, temperature_mean (або temperature),\n",
    "    wind_speed_max (або wind_speed), precipitation, snowfall, etc.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        # Спробуємо завантажити з підготовленого датасету GNN_AGENT\n",
    "        try:\n",
    "            from pathlib import Path\n",
    "            data_dir = Path(\"data\")\n",
    "            # Можна використати готовий файл з GNN_AGENT після об'єднання з погодою\n",
    "            # Або завантажити base_df після об'єднання з погодою\n",
    "            df = pd.read_csv(data_dir / \"combined_region_electricity.csv\", parse_dates=[\"date\"])\n",
    "            # Нормалізуємо назву колонки регіону якщо потрібно\n",
    "            if \"ua_region\" in df.columns and \"region\" not in df.columns:\n",
    "                df = df.rename(columns={\"ua_region\": \"region\"})\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"Файл не знайдено, використовуємо приклад даних\")\n",
    "            return create_sample_risk_data()\n",
    "    \n",
    "    df = pd.read_csv(file_path, parse_dates=[\"date\"])\n",
    "    # Нормалізуємо назву колонки регіону якщо потрібно\n",
    "    if \"ua_region\" in df.columns and \"region\" not in df.columns:\n",
    "        df = df.rename(columns={\"ua_region\": \"region\"})\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_energy_data(file_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Завантажує енергетичні дані з LOAD_AGENT_PREPARE\n",
    "    Очікує колонки: ua_region (або region), timestamp_utc (або date),\n",
    "    demand_ua_adj (або demand), generation_ua_adj (або net_generation),\n",
    "    energy_deficit_adj (або total_interchange)\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        # Завантажуємо з готового файлу LOAD_AGENT_PREPARE\n",
    "        try:\n",
    "            from pathlib import Path\n",
    "            data_dir = Path(\"data\")\n",
    "            df = pd.read_csv(data_dir / \"ready2use_region_electricity.csv\", parse_dates=[\"timestamp_utc\"])\n",
    "            \n",
    "            # Нормалізуємо назви колонок\n",
    "            if \"ua_region\" in df.columns and \"region\" not in df.columns:\n",
    "                df = df.rename(columns={\"ua_region\": \"region\"})\n",
    "            \n",
    "            # Створюємо колонку date з timestamp_utc\n",
    "            if \"date\" not in df.columns and \"timestamp_utc\" in df.columns:\n",
    "                df[\"date\"] = pd.to_datetime(df[\"timestamp_utc\"]).dt.date\n",
    "            \n",
    "            # Маппінг назв колонок до очікуваних\n",
    "            if \"demand_ua_adj\" in df.columns and \"demand\" not in df.columns:\n",
    "                df[\"demand\"] = df[\"demand_ua_adj\"]\n",
    "            if \"generation_ua_adj\" in df.columns and \"net_generation\" not in df.columns:\n",
    "                df[\"net_generation\"] = df[\"generation_ua_adj\"]\n",
    "            if \"energy_deficit_adj\" in df.columns and \"total_interchange\" not in df.columns:\n",
    "                # Дефіцит означає негативний interchange\n",
    "                df[\"total_interchange\"] = -df[\"energy_deficit_adj\"]\n",
    "            \n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"Файл не знайдено, використовуємо приклад даних\")\n",
    "            return create_sample_energy_data()\n",
    "    \n",
    "    df = pd.read_csv(file_path, parse_dates=[\"date\"])\n",
    "    # Нормалізуємо назви колонок якщо потрібно\n",
    "    if \"ua_region\" in df.columns and \"region\" not in df.columns:\n",
    "        df = df.rename(columns={\"ua_region\": \"region\"})\n",
    "    if \"demand_ua_adj\" in df.columns and \"demand\" not in df.columns:\n",
    "        df[\"demand\"] = df[\"demand_ua_adj\"]\n",
    "    if \"generation_ua_adj\" in df.columns and \"net_generation\" not in df.columns:\n",
    "        df[\"net_generation\"] = df[\"generation_ua_adj\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sample_risk_data() -> pd.DataFrame:\n",
    "    \"\"\"Створює приклад даних ризику для тестування\"\"\"\n",
    "    dates = pd.date_range(start=\"2024-01-01\", end=\"2024-01-10\", freq=\"D\")\n",
    "    regions = CANONICAL_REGIONS[:5]  # Перші 5 регіонів\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for region in regions:\n",
    "            data.append({\n",
    "                \"region\": region,\n",
    "                \"date\": date,\n",
    "                \"Alerts\": np.random.randint(0, 3),\n",
    "                \"isDamaged\": np.random.choice([True, False], p=[0.1, 0.9]),\n",
    "                \"temperature_mean\": np.random.uniform(-5, 15),\n",
    "                \"wind_speed_max\": np.random.uniform(5, 25),\n",
    "                \"precipitation\": np.random.uniform(0, 20),\n",
    "                \"number_blackout_consumers\": np.random.randint(0, 5000) if np.random.random() < 0.2 else 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def create_sample_energy_data() -> pd.DataFrame:\n",
    "    \"\"\"Створює приклад енергетичних даних для тестування\"\"\"\n",
    "    dates = pd.date_range(start=\"2024-01-01\", end=\"2024-01-10\", freq=\"D\")\n",
    "    regions = CANONICAL_REGIONS[:5]\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for region in regions:\n",
    "            data.append({\n",
    "                \"region\": region,\n",
    "                \"date\": date,\n",
    "                \"consumption_kwh\": np.random.uniform(10000, 30000),\n",
    "                \"demand\": np.random.uniform(10000, 30000),\n",
    "                \"net_generation\": np.random.uniform(8000, 28000),\n",
    "                \"total_interchange\": np.random.uniform(-2000, 2000)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_data(\n",
    "    risk_df: pd.DataFrame,\n",
    "    energy_df: pd.DataFrame,\n",
    "    target_timestamp: Optional[datetime] = None\n",
    ") -> Dict[str, CanonicalState]:\n",
    "    \"\"\"\n",
    "    Синхронізує дані з різних джерел у канонічний стан\n",
    "    \n",
    "    Args:\n",
    "        risk_df: DataFrame з даними ризику\n",
    "        energy_df: DataFrame з енергетичними даними\n",
    "        target_timestamp: Цільовий момент часу (за замовчуванням - останній доступний)\n",
    "    \n",
    "    Returns:\n",
    "        Словник канонічних станів по регіонах\n",
    "    \"\"\"\n",
    "    canonical_states = {}\n",
    "    \n",
    "    # Нормалізуємо формат дат та назв колонок\n",
    "    # Для risk_df\n",
    "    if \"date\" not in risk_df.columns and \"timestamp_utc\" in risk_df.columns:\n",
    "        risk_df[\"date\"] = pd.to_datetime(risk_df[\"timestamp_utc\"]).dt.date\n",
    "    else:\n",
    "        risk_df[\"date\"] = pd.to_datetime(risk_df[\"date\"]).dt.date\n",
    "    \n",
    "    # Нормалізуємо назву колонки регіону в risk_df\n",
    "    if \"ua_region\" in risk_df.columns and \"region\" not in risk_df.columns:\n",
    "        risk_df[\"region\"] = risk_df[\"ua_region\"]\n",
    "    \n",
    "    # Для energy_df\n",
    "    if \"date\" not in energy_df.columns and \"timestamp_utc\" in energy_df.columns:\n",
    "        energy_df[\"date\"] = pd.to_datetime(energy_df[\"timestamp_utc\"]).dt.date\n",
    "    else:\n",
    "        energy_df[\"date\"] = pd.to_datetime(energy_df[\"date\"]).dt.date\n",
    "    \n",
    "    # Нормалізуємо назву колонки регіону в energy_df\n",
    "    if \"ua_region\" in energy_df.columns and \"region\" not in energy_df.columns:\n",
    "        energy_df[\"region\"] = energy_df[\"ua_region\"]\n",
    "    \n",
    "    # Визначаємо цільовий timestamp\n",
    "    if target_timestamp is None:\n",
    "        all_dates = set(risk_df[\"date\"].unique()) | set(energy_df[\"date\"].unique())\n",
    "        target_date = max(all_dates) if all_dates else date.today()\n",
    "    else:\n",
    "        target_date = target_timestamp.date() if isinstance(target_timestamp, datetime) else target_timestamp\n",
    "    \n",
    "    # Обробляємо кожен регіон\n",
    "    for region in CANONICAL_REGIONS:\n",
    "        # Фільтруємо дані по регіону та даті\n",
    "        risk_region = risk_df[\n",
    "            (risk_df[\"region\"] == region) & \n",
    "            (risk_df[\"date\"] == target_date)\n",
    "        ]\n",
    "        energy_region = energy_df[\n",
    "            (energy_df[\"region\"] == region) & \n",
    "            (energy_df[\"date\"] == target_date)\n",
    "        ]\n",
    "        \n",
    "        # Створюємо канонічний стан\n",
    "        state = create_canonical_state(region, target_date, risk_region, energy_region)\n",
    "        key = f\"{region}_{target_date.isoformat()}\"\n",
    "        canonical_states[key] = state\n",
    "    \n",
    "    return canonical_states\n",
    "\n",
    "\n",
    "def create_canonical_state(\n",
    "    region: str,\n",
    "    timestamp: date,\n",
    "    risk_data: pd.DataFrame,\n",
    "    energy_data: pd.DataFrame\n",
    ") -> CanonicalState:\n",
    "    \"\"\"\n",
    "    Створює канонічний стан з даних ризику та енергетики\n",
    "    \"\"\"\n",
    "    # Ініціалізуємо стан значеннями за замовчуванням\n",
    "    state: CanonicalState = {\n",
    "        \"region\": region,\n",
    "        \"timestamp\": timestamp.isoformat(),\n",
    "        \"air_raid_alert\": 0,\n",
    "        \"alert_intensity\": 0.0,\n",
    "        \"alert_duration_minutes\": None,\n",
    "        \"blackout_status\": \"none\",\n",
    "        \"blackout_probability\": 0.0,\n",
    "        \"blackout_consumers\": 0,\n",
    "        \"blackout_settlements\": 0,\n",
    "        \"weather_temp\": None,\n",
    "        \"weather_wind_speed\": None,\n",
    "        \"weather_wind_gusts\": None,\n",
    "        \"weather_precipitation\": None,\n",
    "        \"weather_snowfall\": None,\n",
    "        \"weather_cloud_cover\": None,\n",
    "        \"weather_pressure\": None,\n",
    "        \"weather_extreme_flag\": False,\n",
    "        \"demand\": None,\n",
    "        \"net_generation\": None,\n",
    "        \"total_interchange\": None,\n",
    "        \"demand_rolling_mean\": None,\n",
    "        \"demand_volatility\": None,\n",
    "        \"generation_rolling_mean\": None,\n",
    "        \"generation_volatility\": None,\n",
    "        \"data_completeness\": 0.0,\n",
    "        \"last_updated\": datetime.now().isoformat(),\n",
    "        \"source_priority\": {}\n",
    "    }\n",
    "    \n",
    "    # Заповнюємо дані ризику\n",
    "    if not risk_data.empty:\n",
    "        row = risk_data.iloc[0]\n",
    "        \n",
    "        # Тривоги - обробляємо різні назви колонок (Alerts або alert_active)\n",
    "        alerts_value = row.get(\"Alerts\", row.get(\"alert_active\", 0))\n",
    "        if isinstance(alerts_value, bool):\n",
    "            alerts_value = 1 if alerts_value else 0\n",
    "        state[\"air_raid_alert\"] = int(alerts_value > 0)\n",
    "        state[\"alert_intensity\"] = min(float(alerts_value) / 10.0, 1.0)\n",
    "        \n",
    "        # Відключення\n",
    "        blackout_consumers = row.get(\"number_blackout_consumers\", 0)\n",
    "        if blackout_consumers > 0:\n",
    "            state[\"blackout_status\"] = \"unplanned\"\n",
    "            state[\"blackout_consumers\"] = int(blackout_consumers)\n",
    "            state[\"blackout_probability\"] = min(blackout_consumers / 10000.0, 1.0)\n",
    "        \n",
    "        # Погода - обробляємо різні назви колонок\n",
    "        # temperature_mean (GNN) або temperature (LOAD)\n",
    "        temp_value = row.get(\"temperature_mean\", row.get(\"temperature\", np.nan))\n",
    "        state[\"weather_temp\"] = float(temp_value) if pd.notna(temp_value) else None\n",
    "        \n",
    "        # wind_speed_max (GNN) або wind_speed (LOAD)\n",
    "        wind_speed_value = row.get(\"wind_speed_max\", row.get(\"wind_speed\", np.nan))\n",
    "        state[\"weather_wind_speed\"] = float(wind_speed_value) if pd.notna(wind_speed_value) else None\n",
    "        \n",
    "        # wind_gusts_max (GNN) або wind_gusts (LOAD)\n",
    "        wind_gusts_value = row.get(\"wind_gusts_max\", row.get(\"wind_gusts\", np.nan))\n",
    "        state[\"weather_wind_gusts\"] = float(wind_gusts_value) if pd.notna(wind_gusts_value) else None\n",
    "        \n",
    "        # precipitation\n",
    "        precip_value = row.get(\"precipitation\", np.nan)\n",
    "        state[\"weather_precipitation\"] = float(precip_value) if pd.notna(precip_value) else None\n",
    "        \n",
    "        # snowfall\n",
    "        snow_value = row.get(\"snowfall\", np.nan)\n",
    "        state[\"weather_snowfall\"] = float(snow_value) if pd.notna(snow_value) else None\n",
    "        \n",
    "        # cloud_cover_mean (GNN) або cloud_cover (LOAD)\n",
    "        cloud_value = row.get(\"cloud_cover_mean\", row.get(\"cloud_cover\", np.nan))\n",
    "        state[\"weather_cloud_cover\"] = float(cloud_value) if pd.notna(cloud_value) else None\n",
    "        \n",
    "        # surface_pressure_mean (GNN) або surface_pressure (LOAD)\n",
    "        pressure_value = row.get(\"surface_pressure_mean\", row.get(\"surface_pressure\", np.nan))\n",
    "        state[\"weather_pressure\"] = float(pressure_value) if pd.notna(pressure_value) else None\n",
    "        \n",
    "        # Екстремальні погодні умови\n",
    "        if state[\"weather_wind_speed\"] and state[\"weather_wind_speed\"] > 25:\n",
    "            state[\"weather_extreme_flag\"] = True\n",
    "        if state[\"weather_wind_gusts\"] and state[\"weather_wind_gusts\"] > 30:\n",
    "            state[\"weather_extreme_flag\"] = True\n",
    "    \n",
    "    # Заповнюємо енергетичні дані - обробляємо різні назви колонок\n",
    "    if not energy_data.empty:\n",
    "        row = energy_data.iloc[0]\n",
    "        \n",
    "        # demand або demand_ua_adj\n",
    "        demand_value = row.get(\"demand\", row.get(\"demand_ua_adj\", np.nan))\n",
    "        state[\"demand\"] = float(demand_value) if pd.notna(demand_value) else None\n",
    "        \n",
    "        # net_generation або generation_ua_adj\n",
    "        gen_value = row.get(\"net_generation\", row.get(\"generation_ua_adj\", np.nan))\n",
    "        state[\"net_generation\"] = float(gen_value) if pd.notna(gen_value) else None\n",
    "        \n",
    "        # total_interchange або energy_deficit_adj (з протилежним знаком)\n",
    "        interchange_value = row.get(\"total_interchange\", None)\n",
    "        if interchange_value is None or pd.isna(interchange_value):\n",
    "            # Якщо немає interchange, використовуємо energy_deficit як індикатор\n",
    "            deficit_value = row.get(\"energy_deficit_adj\", np.nan)\n",
    "            if pd.notna(deficit_value):\n",
    "                # Дефіцит означає негативний interchange\n",
    "                state[\"total_interchange\"] = -float(deficit_value)\n",
    "            else:\n",
    "                state[\"total_interchange\"] = None\n",
    "        else:\n",
    "            state[\"total_interchange\"] = float(interchange_value) if pd.notna(interchange_value) else None\n",
    "    \n",
    "    # Обчислюємо повноту даних\n",
    "    fields = [\n",
    "        state[\"air_raid_alert\"] is not None,\n",
    "        state[\"blackout_status\"] != \"none\",\n",
    "        state[\"weather_temp\"] is not None,\n",
    "        state[\"demand\"] is not None,\n",
    "        state[\"net_generation\"] is not None\n",
    "    ]\n",
    "    state[\"data_completeness\"] = sum(fields) / len(fields)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Функції синхронізації готові\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_energy_graph(canonical_states: Dict[str, CanonicalState]) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Будує граф енергосистеми на основі канонічних станів\n",
    "    \n",
    "    Вузли: регіони\n",
    "    Ребра: зв'язки між регіонами з ВАГАМИ (вага = відстань + енергетичний потік)\n",
    "    Атрибути: стан кожного регіону\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Створюємо словник станів для швидкого доступу\n",
    "    states_by_region = {s[\"region\"]: s for s in canonical_states.values()}\n",
    "    \n",
    "    # Додаємо вузли (регіони)\n",
    "    for key, state in canonical_states.items():\n",
    "        region = state[\"region\"]\n",
    "        \n",
    "        # Обчислюємо критичність регіону\n",
    "        criticality = calculate_region_criticality(state)\n",
    "        \n",
    "        G.add_node(\n",
    "            region,\n",
    "            node_type=\"region\",\n",
    "            criticality=criticality,\n",
    "            blackout_status=state[\"blackout_status\"],\n",
    "            alert_active=state[\"air_raid_alert\"] > 0,\n",
    "            demand=state[\"demand\"],\n",
    "            generation=state[\"net_generation\"],\n",
    "            data_completeness=state[\"data_completeness\"]\n",
    "        )\n",
    "    \n",
    "    # Додаємо ребра між сусідніми регіонами з ВАГАМИ\n",
    "    for i, region1 in enumerate(CANONICAL_REGIONS):\n",
    "        if region1 not in G:\n",
    "            continue\n",
    "        \n",
    "        state1 = states_by_region.get(region1)\n",
    "        if not state1:\n",
    "            continue\n",
    "        \n",
    "        for region2 in CANONICAL_REGIONS[i+1:]:\n",
    "            if region2 not in G:\n",
    "                continue\n",
    "            \n",
    "            state2 = states_by_region.get(region2)\n",
    "            if not state2:\n",
    "                continue\n",
    "            \n",
    "            # Обчислюємо відстань між регіонами\n",
    "            if region1 in REGION_COORDS and region2 in REGION_COORDS:\n",
    "                dist_km = haversine_distance(\n",
    "                    REGION_COORDS[region1],\n",
    "                    REGION_COORDS[region2]\n",
    "                )\n",
    "                \n",
    "                # Додаємо ребро якщо регіони близькі (< 300 км)\n",
    "                if dist_km < 300:\n",
    "                    # Обчислюємо вагу ребра (комбінація відстані та енергетичного потоку)\n",
    "                    weight = calculate_edge_weight(\n",
    "                        state1, state2, dist_km\n",
    "                    )\n",
    "                    \n",
    "                    # Обчислюємо інтенсивність зв'язку (обернено пропорційна відстані)\n",
    "                    connection_intensity = 1.0 / (1.0 + dist_km / 100.0)\n",
    "                    \n",
    "                    # Енергетичний потік (якщо є дані про interchange)\n",
    "                    energy_flow = calculate_energy_flow(state1, state2)\n",
    "                    \n",
    "                    G.add_edge(\n",
    "                        region1,\n",
    "                        region2,\n",
    "                        distance_km=dist_km,\n",
    "                        weight=weight,  # ВАГА РЕБРА (для алгоритмів графів)\n",
    "                        connection_intensity=connection_intensity,\n",
    "                        energy_flow=energy_flow,\n",
    "                        edge_type=\"geographic\"\n",
    "                    )\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def calculate_edge_weight(\n",
    "    state1: CanonicalState,\n",
    "    state2: CanonicalState,\n",
    "    distance_km: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Обчислює вагу ребра між двома регіонами\n",
    "    \n",
    "    Вага враховує:\n",
    "    - Географічну відстань (більша відстань = більша вага)\n",
    "    - Енергетичний потік (більший потік = менша вага, тобто сильніший зв'язок)\n",
    "    - Критичність регіонів (критичні регіони мають меншу вагу для швидшого поширення)\n",
    "    \n",
    "    Returns:\n",
    "        Вага ребра (більша вага = слабший зв'язок)\n",
    "    \"\"\"\n",
    "    # Базова вага від відстані (нормалізуємо до 0-1)\n",
    "    base_weight = min(distance_km / 300.0, 1.0)\n",
    "    \n",
    "    # Корекція від енергетичного потоку\n",
    "    flow_factor = 1.0\n",
    "    if state1.get(\"total_interchange\") and state2.get(\"total_interchange\"):\n",
    "        # Якщо є значний потік між регіонами - зменшуємо вагу\n",
    "        flow_magnitude = abs(state1[\"total_interchange\"]) + abs(state2[\"total_interchange\"])\n",
    "        if flow_magnitude > 1000:  # Значний потік\n",
    "            flow_factor = 0.7  # Зменшуємо вагу на 30%\n",
    "    \n",
    "    # Корекція від критичності (критичні регіони мають пріоритет)\n",
    "    criticality1 = calculate_region_criticality(state1)\n",
    "    criticality2 = calculate_region_criticality(state2)\n",
    "    avg_criticality = (criticality1 + criticality2) / 2.0\n",
    "    \n",
    "    # Якщо середня критичність висока - зменшуємо вагу (сильніший зв'язок)\n",
    "    criticality_factor = 1.0 - (avg_criticality * 0.3)\n",
    "    \n",
    "    # Фінальна вага\n",
    "    final_weight = base_weight * flow_factor * criticality_factor\n",
    "    \n",
    "    return max(final_weight, 0.1)  # Мінімальна вага 0.1\n",
    "\n",
    "\n",
    "def calculate_energy_flow(\n",
    "    state1: CanonicalState,\n",
    "    state2: CanonicalState\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Обчислює енергетичний потік між двома регіонами\n",
    "    \n",
    "    Returns:\n",
    "        Величина потоку (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    if not state1.get(\"total_interchange\") or not state2.get(\"total_interchange\"):\n",
    "        return 0.0\n",
    "    \n",
    "    # Нормалізуємо потік\n",
    "    flow1 = abs(state1[\"total_interchange\"])\n",
    "    flow2 = abs(state2[\"total_interchange\"])\n",
    "    \n",
    "    # Середній потік нормалізуємо до 0-1\n",
    "    avg_flow = (flow1 + flow2) / 2.0\n",
    "    normalized_flow = min(avg_flow / 5000.0, 1.0)  # 5000 як максимальний потік\n",
    "    \n",
    "    return normalized_flow\n",
    "\n",
    "\n",
    "def haversine_distance(coord1: tuple, coord2: tuple) -> float:\n",
    "    \"\"\"Обчислює відстань між двома координатами (км)\"\"\"\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "    \n",
    "    lat1, lon1 = radians(coord1[0]), radians(coord1[1])\n",
    "    lat2, lon2 = radians(coord2[0]), radians(coord2[1])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    R = 6371  # Радіус Землі в км\n",
    "    return R * c\n",
    "\n",
    "\n",
    "def calculate_region_criticality(state: CanonicalState) -> float:\n",
    "    \"\"\"\n",
    "    Обчислює критичність регіону (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Фактор 1: Тривоги\n",
    "    if state[\"air_raid_alert\"] > 0:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Фактор 2: Відключення\n",
    "    if state[\"blackout_status\"] != \"none\":\n",
    "        score += 0.4\n",
    "        score += min(state[\"blackout_probability\"], 0.2)\n",
    "    \n",
    "    # Фактор 3: Екстремальна погода\n",
    "    if state[\"weather_extreme_flag\"]:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Фактор 4: Дисбаланс енергії\n",
    "    if state[\"demand\"] and state[\"net_generation\"]:\n",
    "        imbalance = abs(state[\"demand\"] - state[\"net_generation\"]) / max(state[\"demand\"], 1)\n",
    "        score += min(imbalance * 0.1, 0.1)\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "print(\"Функції побудови графа готові\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_call_llm(\n",
    "    canonical_states: Dict[str, CanonicalState],\n",
    "    energy_graph: nx.Graph\n",
    ") -> Tuple[bool, List[str], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Визначає чи потрібно викликати LLM та формує контекст\n",
    "    ВИКОРИСТОВУЄ ВАГИ РЕБЕР для аналізу поширення критичності по графу\n",
    "    \n",
    "    Returns:\n",
    "        (should_call, critical_regions, context)\n",
    "    \"\"\"\n",
    "    critical_regions = []\n",
    "    context = {\n",
    "        \"summary\": {},\n",
    "        \"critical_events\": [],\n",
    "        \"graph_stats\": {},\n",
    "        \"graph_metrics\": {}  # Метрики графа з урахуванням ваг\n",
    "    }\n",
    "    \n",
    "    # Аналізуємо критичність регіонів\n",
    "    for key, state in canonical_states.items():\n",
    "        criticality = calculate_region_criticality(state)\n",
    "        \n",
    "        if criticality > 0.5:  # Поріг критичності\n",
    "            critical_regions.append(state[\"region\"])\n",
    "            context[\"critical_events\"].append({\n",
    "                \"region\": state[\"region\"],\n",
    "                \"criticality\": criticality,\n",
    "                \"issues\": [\n",
    "                    f\"Тривоги: {state['air_raid_alert']}\" if state[\"air_raid_alert\"] > 0 else None,\n",
    "                    f\"Відключення: {state['blackout_status']}\" if state[\"blackout_status\"] != \"none\" else None,\n",
    "                    \"Екстремальна погода\" if state[\"weather_extreme_flag\"] else None\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # АНАЛІЗ ГРАФА З ВАГАМИ РЕБЕР\n",
    "    if energy_graph and energy_graph.number_of_edges() > 0:\n",
    "        # 1. Обчислюємо найкоротші шляхи з урахуванням ваг\n",
    "        shortest_paths_analysis = analyze_shortest_paths(energy_graph, critical_regions)\n",
    "        \n",
    "        # 2. Обчислюємо центральність з урахуванням ваг\n",
    "        centrality_metrics = calculate_weighted_centrality(energy_graph)\n",
    "        \n",
    "        # 3. Аналізуємо поширення критичності по графу\n",
    "        criticality_propagation = analyze_criticality_propagation(\n",
    "            energy_graph, canonical_states\n",
    "        )\n",
    "        \n",
    "        # 4. Знаходимо найбільш вразливі регіони (висока центральність + низька вага зв'язків)\n",
    "        vulnerable_regions = find_vulnerable_regions(\n",
    "            energy_graph, centrality_metrics\n",
    "        )\n",
    "        \n",
    "        context[\"graph_stats\"] = {\n",
    "            \"total_nodes\": energy_graph.number_of_nodes(),\n",
    "            \"total_edges\": energy_graph.number_of_edges(),\n",
    "            \"critical_nodes\": len([n for n, d in energy_graph.nodes(data=True) if d.get(\"criticality\", 0) > 0.5]),\n",
    "            \"average_edge_weight\": np.mean([\n",
    "                d.get(\"weight\", 1.0) for u, v, d in energy_graph.edges(data=True)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        context[\"graph_metrics\"] = {\n",
    "            \"shortest_paths\": shortest_paths_analysis,\n",
    "            \"centrality\": centrality_metrics,\n",
    "            \"criticality_propagation\": criticality_propagation,\n",
    "            \"vulnerable_regions\": vulnerable_regions\n",
    "        }\n",
    "    else:\n",
    "        context[\"graph_stats\"] = {\n",
    "            \"total_nodes\": energy_graph.number_of_nodes() if energy_graph else 0,\n",
    "            \"total_edges\": 0,\n",
    "            \"critical_nodes\": 0,\n",
    "            \"average_edge_weight\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Загальна статистика\n",
    "    total_regions = len(canonical_states)\n",
    "    context[\"summary\"] = {\n",
    "        \"total_regions\": total_regions,\n",
    "        \"critical_regions_count\": len(critical_regions),\n",
    "        \"average_criticality\": np.mean([calculate_region_criticality(s) for s in canonical_states.values()]),\n",
    "        \"data_completeness\": np.mean([s[\"data_completeness\"] for s in canonical_states.values()])\n",
    "    }\n",
    "    \n",
    "    # ВИРІШЕННЯ ЧИ ВИКЛИКАТИ LLM з урахуванням ваг ребер\n",
    "    should_call = determine_llm_call_with_weights(\n",
    "        critical_regions,\n",
    "        context,\n",
    "        energy_graph\n",
    "    )\n",
    "    \n",
    "    return should_call, critical_regions, context\n",
    "\n",
    "\n",
    "def analyze_shortest_paths(\n",
    "    graph: nx.Graph,\n",
    "    critical_regions: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Аналізує найкоротші шляхи від критичних регіонів до інших\n",
    "    Використовує ваги ребер (weight)\n",
    "    \"\"\"\n",
    "    if not critical_regions or graph.number_of_nodes() == 0:\n",
    "        return {\"max_path_length\": 0, \"affected_regions\": []}\n",
    "    \n",
    "    # Знаходимо найкоротші шляхи від кожного критичного регіону\n",
    "    all_paths = []\n",
    "    affected_regions = set()\n",
    "    \n",
    "    for critical_region in critical_regions:\n",
    "        if critical_region not in graph:\n",
    "            continue\n",
    "        \n",
    "        # Використовуємо Dijkstra з вагами\n",
    "        try:\n",
    "            paths = nx.single_source_dijkstra_path_length(\n",
    "                graph, critical_region, weight=\"weight\"\n",
    "            )\n",
    "            \n",
    "            # Регіони на відстані < 2.0 (з урахуванням ваг)\n",
    "            for region, path_length in paths.items():\n",
    "                if path_length < 2.0 and region != critical_region:\n",
    "                    affected_regions.add(region)\n",
    "                    all_paths.append({\n",
    "                        \"from\": critical_region,\n",
    "                        \"to\": region,\n",
    "                        \"weighted_distance\": path_length\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Помилка обчислення шляхів для {critical_region}: {e}\")\n",
    "    \n",
    "    max_path_length = max([p[\"weighted_distance\"] for p in all_paths]) if all_paths else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"max_path_length\": max_path_length,\n",
    "        \"affected_regions\": list(affected_regions),\n",
    "        \"paths_count\": len(all_paths)\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_weighted_centrality(graph: nx.Graph) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Обчислює центральність вузлів з урахуванням ваг ребер\n",
    "    \"\"\"\n",
    "    if graph.number_of_nodes() == 0:\n",
    "        return {}\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    try:\n",
    "        # Betweenness centrality з вагами (вага = відстань)\n",
    "        betweenness = nx.betweenness_centrality(graph, weight=\"weight\")\n",
    "        \n",
    "        # Closeness centrality з вагами\n",
    "        closeness = nx.closeness_centrality(graph, distance=\"weight\")\n",
    "        \n",
    "        # Degree centrality (проста, без ваг)\n",
    "        degree = nx.degree_centrality(graph)\n",
    "        \n",
    "        for node in graph.nodes():\n",
    "            centrality[node] = {\n",
    "                \"betweenness\": betweenness.get(node, 0.0),\n",
    "                \"closeness\": closeness.get(node, 0.0),\n",
    "                \"degree\": degree.get(node, 0.0),\n",
    "                \"combined\": (\n",
    "                    betweenness.get(node, 0.0) * 0.4 +\n",
    "                    closeness.get(node, 0.0) * 0.4 +\n",
    "                    degree.get(node, 0.0) * 0.2\n",
    "                )\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Помилка обчислення центральності: {e}\")\n",
    "        # Fallback: проста центральність без ваг\n",
    "        degree = nx.degree_centrality(graph)\n",
    "        for node in graph.nodes():\n",
    "            centrality[node] = {\n",
    "                \"betweenness\": 0.0,\n",
    "                \"closeness\": 0.0,\n",
    "                \"degree\": degree.get(node, 0.0),\n",
    "                \"combined\": degree.get(node, 0.0)\n",
    "            }\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "\n",
    "def analyze_criticality_propagation(\n",
    "    graph: nx.Graph,\n",
    "    canonical_states: Dict[str, CanonicalState]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Аналізує як критичність поширюється по графу з урахуванням ваг ребер\n",
    "    \"\"\"\n",
    "    if graph.number_of_edges() == 0:\n",
    "        return {\"propagation_score\": 0.0, \"risk_zones\": []}\n",
    "    \n",
    "    states_by_region = {s[\"region\"]: s for s in canonical_states.values()}\n",
    "    risk_zones = []\n",
    "    total_propagation = 0.0\n",
    "    \n",
    "    # Для кожного критичного регіону обчислюємо зону ризику\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        criticality = data.get(\"criticality\", 0.0)\n",
    "        \n",
    "        if criticality > 0.3:  # Регіон з підвищеною критичністю\n",
    "            # Знаходимо сусідів та обчислюємо вплив через ваги ребер\n",
    "            neighbors_risk = []\n",
    "            \n",
    "            for neighbor in graph.neighbors(node):\n",
    "                edge_data = graph.get_edge_data(node, neighbor)\n",
    "                weight = edge_data.get(\"weight\", 1.0)\n",
    "                \n",
    "                neighbor_criticality = graph.nodes[neighbor].get(\"criticality\", 0.0)\n",
    "                \n",
    "                # Вплив зменшується зі збільшенням ваги (відстані)\n",
    "                influence = neighbor_criticality / (1.0 + weight)\n",
    "                neighbors_risk.append({\n",
    "                    \"neighbor\": neighbor,\n",
    "                    \"influence\": influence,\n",
    "                    \"edge_weight\": weight\n",
    "                })\n",
    "            \n",
    "            # Сумарний ризик поширення\n",
    "            propagation_score = criticality + sum(n[\"influence\"] for n in neighbors_risk)\n",
    "            \n",
    "            risk_zones.append({\n",
    "                \"region\": node,\n",
    "                \"base_criticality\": criticality,\n",
    "                \"propagation_score\": propagation_score,\n",
    "                \"neighbors_count\": len(neighbors_risk)\n",
    "            })\n",
    "            \n",
    "            total_propagation += propagation_score\n",
    "    \n",
    "    avg_propagation = total_propagation / len(risk_zones) if risk_zones else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"propagation_score\": avg_propagation,\n",
    "        \"risk_zones\": risk_zones[:10],  # Топ-10 зон ризику\n",
    "        \"total_risk_zones\": len(risk_zones)\n",
    "    }\n",
    "\n",
    "\n",
    "def find_vulnerable_regions(\n",
    "    graph: nx.Graph,\n",
    "    centrality_metrics: Dict[str, Dict[str, float]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Знаходить найбільш вразливі регіони:\n",
    "    - Висока центральність (важливі для системи)\n",
    "    - Низька середня вага зв'язків (легко поширюється проблема)\n",
    "    \"\"\"\n",
    "    vulnerable = []\n",
    "    \n",
    "    for node in graph.nodes():\n",
    "        centrality = centrality_metrics.get(node, {})\n",
    "        combined_centrality = centrality.get(\"combined\", 0.0)\n",
    "        \n",
    "        # Обчислюємо середню вагу зв'язків цього вузла\n",
    "        edges = list(graph.edges(node, data=True))\n",
    "        if edges:\n",
    "            avg_edge_weight = np.mean([\n",
    "                d.get(\"weight\", 1.0) for u, v, d in edges\n",
    "            ])\n",
    "            \n",
    "            # Вразливість = висока центральність + низька вага зв'язків\n",
    "            vulnerability = combined_centrality * (1.0 / (1.0 + avg_edge_weight))\n",
    "            \n",
    "            vulnerable.append({\n",
    "                \"region\": node,\n",
    "                \"centrality\": combined_centrality,\n",
    "                \"avg_edge_weight\": avg_edge_weight,\n",
    "                \"vulnerability\": vulnerability\n",
    "            })\n",
    "    \n",
    "    # Сортуємо за вразливістю\n",
    "    vulnerable.sort(key=lambda x: x[\"vulnerability\"], reverse=True)\n",
    "    \n",
    "    return vulnerable[:10]  # Топ-10 найбільш вразливих\n",
    "\n",
    "\n",
    "def determine_llm_call_with_weights(\n",
    "    critical_regions: List[str],\n",
    "    context: Dict[str, Any],\n",
    "    energy_graph: nx.Graph\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Визначає чи викликати LLM з урахуванням ваг ребер графа\n",
    "    \"\"\"\n",
    "    # Базові умови\n",
    "    basic_conditions = (\n",
    "        len(critical_regions) > 0 or\n",
    "        context[\"summary\"][\"average_criticality\"] > 0.3 or\n",
    "        context[\"summary\"][\"data_completeness\"] < 0.7\n",
    "    )\n",
    "    \n",
    "    if not basic_conditions:\n",
    "        return False\n",
    "    \n",
    "    # Додаткові умови з урахуванням ваг ребер\n",
    "    graph_metrics = context.get(\"graph_metrics\", {})\n",
    "    \n",
    "    # Умова 1: Поширення критичності по графу\n",
    "    propagation = graph_metrics.get(\"criticality_propagation\", {})\n",
    "    if propagation.get(\"propagation_score\", 0.0) > 0.5:\n",
    "        return True\n",
    "    \n",
    "    # Умова 2: Велика кількість регіонів під впливом критичних\n",
    "    shortest_paths = graph_metrics.get(\"shortest_paths\", {})\n",
    "    if shortest_paths.get(\"affected_regions\", []):\n",
    "        affected_count = len(shortest_paths[\"affected_regions\"])\n",
    "        if affected_count > len(critical_regions) * 2:  # Проблема поширюється\n",
    "            return True\n",
    "    \n",
    "    # Умова 3: Високовразливі регіони\n",
    "    vulnerable = graph_metrics.get(\"vulnerable_regions\", [])\n",
    "    if vulnerable:\n",
    "        high_vulnerability = [v for v in vulnerable if v[\"vulnerability\"] > 0.3]\n",
    "        if len(high_vulnerability) > 2:\n",
    "            return True\n",
    "    \n",
    "    return basic_conditions\n",
    "\n",
    "print(\"Функція визначення виклику LLM готова\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sources(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"Вузол 1: Завантаження даних з різних джерел\"\"\"\n",
    "    print(\"[load_data_sources] Завантаження даних...\")\n",
    "    \n",
    "    risk_df = load_risk_data()\n",
    "    energy_df = load_energy_data()\n",
    "    \n",
    "    state[\"risk_data\"] = {\"main\": risk_df}\n",
    "    state[\"energy_data\"] = {\"main\": energy_df}\n",
    "    state[\"sync_timestamp\"] = datetime.now().isoformat()\n",
    "    \n",
    "    print(f\"[load_data_sources] Завантажено: {len(risk_df)} рядків ризику, {len(energy_df)} рядків енергетики\")\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synchronize_sources(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"Вузол 2: Синхронізація даних у канонічний стан\"\"\"\n",
    "    print(\"[synchronize_sources] Синхронізація даних...\")\n",
    "    \n",
    "    risk_df = state[\"risk_data\"][\"main\"]\n",
    "    energy_df = state[\"energy_data\"][\"main\"]\n",
    "    \n",
    "    canonical_states = synchronize_data(risk_df, energy_df)\n",
    "    \n",
    "    state[\"canonical_states\"] = canonical_states\n",
    "    state[\"regions_processed\"] = list(set(s[\"region\"] for s in canonical_states.values()))\n",
    "    \n",
    "    print(f\"[synchronize_sources] Синхронізовано {len(canonical_states)} станів\")\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"Вузол 3: Побудова energy-graph\"\"\"\n",
    "    print(\"[build_graph] Побудова графа енергосистеми...\")\n",
    "    \n",
    "    canonical_states = state[\"canonical_states\"]\n",
    "    energy_graph = build_energy_graph(canonical_states)\n",
    "    \n",
    "    state[\"energy_graph\"] = energy_graph\n",
    "    \n",
    "    print(f\"[build_graph] Граф побудовано: {energy_graph.number_of_nodes()} вузлів, {energy_graph.number_of_edges()} ребер\")\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_llm_call(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"Вузол 4: Визначення чи потрібно викликати LLM\"\"\"\n",
    "    print(\"[decide_llm_call] Оцінка необхідності виклику LLM...\")\n",
    "    \n",
    "    canonical_states = state[\"canonical_states\"]\n",
    "    energy_graph = state[\"energy_graph\"]\n",
    "    \n",
    "    should_call, critical_regions, context = should_call_llm(canonical_states, energy_graph)\n",
    "    \n",
    "    state[\"should_call_llm\"] = should_call\n",
    "    state[\"critical_regions\"] = critical_regions\n",
    "    state[\"llm_context\"] = context\n",
    "    state[\"decision_required\"] = should_call\n",
    "    \n",
    "    print(f\"[decide_llm_call] Виклик LLM: {'ТАК' if should_call else 'НІ'}\")\n",
    "    if critical_regions:\n",
    "        print(f\"[decide_llm_call] Критичні регіони: {', '.join(critical_regions)}\")\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_llm_agent(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"Вузол 5: Виклик LLM-агента з контекстом\"\"\"\n",
    "    print(\"[call_llm_agent] Виклик LLM-агента...\")\n",
    "    \n",
    "    context = state[\"llm_context\"]\n",
    "    canonical_states = state[\"canonical_states\"]\n",
    "    energy_graph = state[\"energy_graph\"]\n",
    "    \n",
    "    # Формуємо промпт для LLM\n",
    "    prompt = format_context_for_llm(context, canonical_states, energy_graph)\n",
    "    \n",
    "    try:\n",
    "        llm = get_openai_client()\n",
    "        \n",
    "        # Тут можна викликати LLM з промптом\n",
    "        # response = llm.invoke(prompt)\n",
    "        # state[\"llm_response\"] = response.content\n",
    "        \n",
    "        print(\"[call_llm_agent] LLM викликано (симуляція)\")\n",
    "        state[\"llm_response\"] = \"LLM response would be here\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[call_llm_agent] Помилка: {e}\")\n",
    "        state[\"llm_response\"] = None\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def format_context_for_llm(\n",
    "    context: Dict[str, Any],\n",
    "    canonical_states: Dict[str, CanonicalState],\n",
    "    energy_graph: nx.Graph\n",
    ") -> str:\n",
    "    \"\"\"Формує текстовий контекст для передачі LLM\"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    # Загальна інформація\n",
    "    parts.append(\"=== СТАН ЕНЕРГОСИСТЕМИ ===\")\n",
    "    parts.append(f\"Загальна кількість регіонів: {context['summary']['total_regions']}\")\n",
    "    parts.append(f\"Критичних регіонів: {context['summary']['critical_regions_count']}\")\n",
    "    parts.append(f\"Середня критичність: {context['summary']['average_criticality']:.2f}\")\n",
    "    parts.append(f\"Повнота даних: {context['summary']['data_completeness']:.2f}\")\n",
    "    \n",
    "    # Критичні події\n",
    "    if context[\"critical_events\"]:\n",
    "        parts.append(\"\\n=== КРИТИЧНІ ПОДІЇ ===\")\n",
    "        for event in context[\"critical_events\"]:\n",
    "            parts.append(f\"Регіон: {event['region']}\")\n",
    "            parts.append(f\"  Критичність: {event['criticality']:.2f}\")\n",
    "            issues = [i for i in event['issues'] if i]\n",
    "            if issues:\n",
    "                parts.append(f\"  Проблеми: {', '.join(issues)}\")\n",
    "    \n",
    "    # Статистика графа\n",
    "    if context[\"graph_stats\"]:\n",
    "        parts.append(\"\\n=== СТРУКТУРА ЕНЕРГОСИСТЕМИ ===\")\n",
    "        parts.append(f\"Вузлів: {context['graph_stats']['total_nodes']}\")\n",
    "        parts.append(f\"Зв'язків: {context['graph_stats']['total_edges']}\")\n",
    "        parts.append(f\"Критичних вузлів: {context['graph_stats']['critical_nodes']}\")\n",
    "    \n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "print(\"Функції виклику LLM готові\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "orchestrator_graph = StateGraph(OrchestratorState)\n",
    "\n",
    "# Додаємо вузли\n",
    "orchestrator_graph.add_node(\"load_data_sources\", load_data_sources)\n",
    "orchestrator_graph.add_node(\"synchronize_sources\", synchronize_sources)\n",
    "orchestrator_graph.add_node(\"build_graph\", build_graph)\n",
    "orchestrator_graph.add_node(\"decide_llm_call\", decide_llm_call)\n",
    "orchestrator_graph.add_node(\"call_llm_agent\", call_llm_agent)\n",
    "\n",
    "# Визначаємо початкову точку\n",
    "orchestrator_graph.set_entry_point(\"load_data_sources\")\n",
    "\n",
    "# Додаємо переходи\n",
    "orchestrator_graph.add_edge(\"load_data_sources\", \"synchronize_sources\")\n",
    "orchestrator_graph.add_edge(\"synchronize_sources\", \"build_graph\")\n",
    "orchestrator_graph.add_edge(\"build_graph\", \"decide_llm_call\")\n",
    "\n",
    "# Умовний перехід: чи викликати LLM\n",
    "def should_call_llm_route(state: OrchestratorState) -> str:\n",
    "    if state.get(\"should_call_llm\", False):\n",
    "        return \"call_llm_agent\"\n",
    "    return END\n",
    "\n",
    "orchestrator_graph.add_conditional_edges(\n",
    "    \"decide_llm_call\",\n",
    "    should_call_llm_route,\n",
    "    {\n",
    "        \"call_llm_agent\": \"call_llm_agent\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "orchestrator_graph.add_edge(\"call_llm_agent\", END)\n",
    "\n",
    "# Компілюємо граф\n",
    "orchestrator_app = orchestrator_graph.compile()\n",
    "\n",
    "print(\"Оркестратор побудовано!\")\n",
    "print(\"\\nСтруктура графа:\")\n",
    "print(\"load_data_sources -> synchronize_sources -> build_graph -> decide_llm_call\")\n",
    "print(\"  -> [call_llm_agent | END]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_orchestrator(target_timestamp: Optional[datetime] = None) -> OrchestratorState:\n",
    "    \"\"\"Запускає оркестратор з вхідними даними\"\"\"\n",
    "    \n",
    "    initial_state: OrchestratorState = {\n",
    "        \"risk_data\": {},\n",
    "        \"energy_data\": {},\n",
    "        \"canonical_states\": {},\n",
    "        \"energy_graph\": None,\n",
    "        \"llm_context\": None,\n",
    "        \"should_call_llm\": False,\n",
    "        \"llm_response\": None,\n",
    "        \"sync_timestamp\": datetime.now().isoformat(),\n",
    "        \"regions_processed\": [],\n",
    "        \"sync_errors\": [],\n",
    "        \"decision_required\": False,\n",
    "        \"critical_regions\": []\n",
    "    }\n",
    " \n",
    "    \n",
    "    final_state = orchestrator_app.invoke(initial_state)\n",
    " \n",
    "    print(f\"Оброблено регіонів: {len(final_state['regions_processed'])}\")\n",
    "    print(f\"Критичних регіонів: {len(final_state['critical_regions'])}\")\n",
    "    print(f\"Виклик LLM: {'ТАК' if final_state['should_call_llm'] else 'НІ'}\")\n",
    "    \n",
    "    if final_state[\"llm_context\"]:\n",
    "        print(f\"\\nКонтекст для LLM:\")\n",
    "        print(f\"  Середня критичність: {final_state['llm_context']['summary']['average_criticality']:.2f}\")\n",
    "        print(f\"  Повнота даних: {final_state['llm_context']['summary']['data_completeness']:.2f}\")\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "# Запускаємо оркестратор\n",
    "result = run_orchestrator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Використання ваг ребер у визначенні виклику LLM\n",
    "\n",
    "### Математичні операції з вагами:\n",
    "\n",
    "1. **Найкоротші шляхи (Dijkstra)**:\n",
    "   - Використовує `weight` як відстань\n",
    "   - Знаходить регіони під впливом критичних\n",
    "   - `nx.single_source_dijkstra_path_length(graph, node, weight=\"weight\")`\n",
    "\n",
    "2. **Центральність з вагами**:\n",
    "   - **Betweenness centrality**: скільки найкоротших шляхів проходить через вузол\n",
    "   - **Closeness centrality**: обернена сума найкоротших відстаней до всіх інших вузлів\n",
    "   - Використовує ваги для обчислення відстаней\n",
    "\n",
    "3. **Поширення критичності**:\n",
    "   - Вплив критичного регіону на сусідів зменшується зі збільшенням ваги ребра\n",
    "   - `influence = neighbor_criticality / (1.0 + weight)`\n",
    "\n",
    "4. **Вразливість регіонів**:\n",
    "   - Комбінація високої центральності та низької ваги зв'язків\n",
    "   - `vulnerability = centrality * (1.0 / (1.0 + avg_edge_weight))`\n",
    "\n",
    "### Умови виклику LLM з вагами:\n",
    "\n",
    "- Поширення критичності > 0.5 (проблема поширюється по графу)\n",
    "- Багато регіонів під впливом критичних (через найкоротші шляхи)\n",
    "- Високовразливі регіони (важливі вузли з низькою вагою зв'язків)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'result' in locals() and result.get(\"energy_graph\"):\n",
    "    graph = result[\"energy_graph\"]\n",
    "  \n",
    "    \n",
    "    if graph.number_of_edges() > 0:\n",
    "        edges_with_weights = list(graph.edges(data=True))\n",
    "        \n",
    "        print(f\"\\nЗагальна кількість ребер: {len(edges_with_weights)}\")\n",
    "        print(f\"\\nСередня вага ребра: {np.mean([d.get('weight', 1.0) for u, v, d in edges_with_weights]):.3f}\")\n",
    "        print(f\"Мінімальна вага: {min([d.get('weight', 1.0) for u, v, d in edges_with_weights]):.3f}\")\n",
    "        print(f\"Максимальна вага: {max([d.get('weight', 1.0) for u, v, d in edges_with_weights]):.3f}\")\n",
    "        \n",
    "        print(\"\\nПриклади ребер з вагами:\")\n",
    "        for u, v, d in edges_with_weights[:5]:\n",
    "            print(f\"  {u} <-> {v}: вага={d.get('weight', 1.0):.3f}, \"\n",
    "                  f\"відстань={d.get('distance_km', 0):.1f}км, \"\n",
    "                  f\"потік={d.get('energy_flow', 0.0):.3f}\")\n",
    "        \n",
    "        # Метрики графа\n",
    "        if result.get(\"graph_metrics\"):\n",
    "            metrics = result[\"graph_metrics\"]\n",
    "            \n",
    "        \n",
    "            \n",
    "            # Найкоротші шляхи\n",
    "            if \"shortest_paths\" in metrics:\n",
    "                sp = metrics[\"shortest_paths\"]\n",
    "                print(f\"\\nНайкоротші шляхи:\")\n",
    "                print(f\"  Максимальна відстань: {sp.get('max_path_length', 0):.3f}\")\n",
    "                print(f\"  Регіонів під впливом: {len(sp.get('affected_regions', []))}\")\n",
    "            \n",
    "            # Центральність\n",
    "            if \"centrality\" in metrics:\n",
    "                centrality = metrics[\"centrality\"]\n",
    "                top_central = sorted(\n",
    "                    centrality.items(),\n",
    "                    key=lambda x: x[1].get(\"combined\", 0.0),\n",
    "                    reverse=True\n",
    "                )[:5]\n",
    "                print(f\"\\nТоп-5 найбільш центральних регіонів:\")\n",
    "                for region, metrics_dict in top_central:\n",
    "                    print(f\"  {region}: combined={metrics_dict['combined']:.3f}, \"\n",
    "                          f\"betweenness={metrics_dict['betweenness']:.3f}\")\n",
    "            \n",
    "            # Поширення критичності\n",
    "            if \"criticality_propagation\" in metrics:\n",
    "                prop = metrics[\"criticality_propagation\"]\n",
    "                print(f\"\\nПоширення критичності:\")\n",
    "                print(f\"  Середній бал поширення: {prop.get('propagation_score', 0):.3f}\")\n",
    "                print(f\"  Зон ризику: {prop.get('total_risk_zones', 0)}\")\n",
    "            \n",
    "            # Вразливі регіони\n",
    "            if \"vulnerable_regions\" in metrics:\n",
    "                vuln = metrics[\"vulnerable_regions\"]\n",
    "                print(f\"\\nТоп-5 найбільш вразливих регіонів:\")\n",
    "                for v in vuln[:5]:\n",
    "                    print(f\"  {v['region']}: вразливість={v['vulnerability']:.3f}, \"\n",
    "                          f\"центральність={v['centrality']:.3f}, \"\n",
    "                          f\"середня вага={v['avg_edge_weight']:.3f}\")\n",
    "    else:\n",
    "        print(\"Граф не містить ребер\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
